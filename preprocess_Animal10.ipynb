{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jLrn6NbZQ-zD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import os\n",
        "import cv2 \n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "from skimage.transform import rotate, AffineTransform\n",
        "from skimage.util import random_noise\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import TensorDataset, ConcatDataset\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8nCBRoTYdle1",
        "outputId": "53738bae-9567-4f58-e92e-8076bde5c80a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "rQfhsRgWSGTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the lack of RAM, we are only reading 1000 images per class. Total 10k"
      ],
      "metadata": {
        "id": "u3Z8IW78LBIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This class reads the data for each dataset\n",
        "class readDataset:\n",
        "    def __init__(self, num_classes, path):\n",
        "        self.height = 200\n",
        "        self.width = 200\n",
        "        self.channels = 3\n",
        "        self.path = path\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def Animal10(self):\n",
        "        class_names = ['cane', 'cavallo', 'elefante',\n",
        "                       'farfalla', 'gallina', 'gatto',\n",
        "                       'mucca', 'pecora', 'ragno', 'scoiattolo']\n",
        "        # list which contains data\n",
        "        data = []\n",
        "        data_labels = []\n",
        "        # read path names of dataset\n",
        "        for i in range(self.num_classes) :\n",
        "            num_data_per_class = 0\n",
        "            path = self.path +class_names[i] \n",
        "            data_names = os.listdir(path)\n",
        "            print(\"\\n class: \",class_names[i])\n",
        "            print(\"number of data: \",len(data_names))\n",
        "            # read whole data\n",
        "            for j in range(len(data_names)):\n",
        "                path_to_image = path + \"/\" + data_names[j]\n",
        "                image = cv2.imread(path_to_image)\n",
        "                image = cv2.resize(image, (self.height, self.width))\n",
        "                data.append(np.array(image))\n",
        "                data_labels.append(i)\n",
        "                # filter over number of images per class\n",
        "                if num_data_per_class ==1000:\n",
        "                    break\n",
        "                num_data_per_class += 1\n",
        "\n",
        "        # return two lists\n",
        "        data = np.array(data)\n",
        "        # encode the labels\n",
        "        data_labels = tf.keras.utils.to_categorical(np.array(data_labels), self.num_classes)\n",
        "\n",
        "        return data, data_labels\n"
      ],
      "metadata": {
        "id": "bScbFrSETP7Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/projectDataset/animal10/raw-img/\"\n",
        "animal10 = readDataset(10, dataset_path)\n",
        "data, data_labels = animal10.Animal10()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bC52dilV_Oq",
        "outputId": "86013363-6c48-4644-e729-932b6f2e6f0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " class:  cane\n",
            "number of data:  5050\n",
            "\n",
            " class:  cavallo\n",
            "number of data:  2630\n",
            "\n",
            " class:  elefante\n",
            "number of data:  1454\n",
            "\n",
            " class:  farfalla\n",
            "number of data:  2123\n",
            "\n",
            " class:  gallina\n",
            "number of data:  3098\n",
            "\n",
            " class:  gatto\n",
            "number of data:  1689\n",
            "\n",
            " class:  mucca\n",
            "number of data:  1898\n",
            "\n",
            " class:  pecora\n",
            "number of data:  1820\n",
            "\n",
            " class:  ragno\n",
            "number of data:  4849\n",
            "\n",
            " class:  scoiattolo\n",
            "number of data:  1862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is saving the train and test data in to the drive\n",
        "path_save = '/content/drive/MyDrive/projectDataset/animal10/numpyFiles/'\n",
        "np.save(path_save+'data.npy', data)\n",
        "np.save(path_save+'data_labels.npy',data_labels)"
      ],
      "metadata": {
        "id": "ElTEDH_3cHQU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is loading the numpies from the drive\n",
        "# uncomment in case\n",
        "\n",
        "# path_save = '/content/drive/MyDrive/projectDataset/dogsAndCats/dataset/numpyFiles/'\n",
        "# train_data = np.load(path_save+'catDogs_train_data.npy')\n",
        "# train_labels = np.load(path_save+'catDogs_train_labels.npy')\n",
        "# test_data = np.load(path_save+'catDogs_test_data.npy')\n",
        "# test_labels = np.load(path_save+'catDogs_test_labels.npy')"
      ],
      "metadata": {
        "id": "SIVWRooPmWjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the data\n",
        "# train_data = train_data/255.0\n",
        "# test_data = test_data/255.0"
      ],
      "metadata": {
        "id": "pFYtE5XTUp1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data to train and test\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, \n",
        "                                                                 data_labels, \n",
        "                                                                 test_size=0.2)\n",
        "# split train to train and val\n",
        "train_data, val_data, train_labels, val_label = train_test_split(train_data, \n",
        "                                                                 train_labels, \n",
        "                                                                 test_size=0.2)"
      ],
      "metadata": {
        "id": "H0YzKplej9q0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Augmentation in Pytorch"
      ],
      "metadata": {
        "id": "bKfdMkodjucn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloader pytorch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "# convert numpy to tensors\n",
        "train_data = torch.from_numpy(train_data.astype(np.float32))\n",
        "test_data = torch.from_numpy(test_data.astype(np.float32))\n",
        "val_data = torch.from_numpy(val_data.astype(np.float32))\n",
        "train_labels = torch.from_numpy(train_labels.astype(np.float32))\n",
        "test_labels = torch.from_numpy(test_labels.astype(np.float32))\n",
        "val_labels = torch.from_numpy(val_label.astype(np.float32))\n",
        "# create tensor dataset\n",
        "train_dataset = TensorDataset(train_data, train_labels)\n",
        "val_dataset = TensorDataset(val_data, val_labels)\n",
        "test_dataset = TensorDataset(test_data, test_labels)"
      ],
      "metadata": {
        "id": "NLqVbcTvjznx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomRotation(degrees=45),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15)),\n",
        "    # transforms.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "papEtZjvjzsY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this list includes k random number within the range of train length\n",
        "train_random_list = random.sample(range(0, len(train_data)), round(0.5 * len(train_data)))\n",
        "val_random_list = random.sample(range(0, len(val_data)), round(0.5 * len(val_data)))"
      ],
      "metadata": {
        "id": "L501_SbKjzul"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply transformations to each sample in the dataset\n",
        "transformed_train = []\n",
        "cnt = 0\n",
        "for sample, label in train_dataset:\n",
        "    if cnt in train_random_list:\n",
        "      transformed_sample = transform(sample.permute(2, 0, 1))\n",
        "      # print(transformed_sample.shape)\n",
        "      transformed_train.append((transformed_sample.permute(1, 2, 0), label))\n",
        "    cnt += 1"
      ],
      "metadata": {
        "id": "DlW8G7hUjzw-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the two datasets\n",
        "augmented_train_dataset = ConcatDataset([train_dataset, transformed_train])"
      ],
      "metadata": {
        "id": "_i4-5GxWkCI7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply transformations to each sample in the val dataset\n",
        "transformed_val = []\n",
        "cnt = 0\n",
        "for sample, label in val_dataset:\n",
        "    if cnt in val_random_list:\n",
        "      transformed_sample = transform(sample.permute(2, 0, 1))\n",
        "      # print(transformed_sample.shape)\n",
        "      transformed_val.append((transformed_sample.permute(1, 2, 0), label))\n",
        "    cnt += 1"
      ],
      "metadata": {
        "id": "pzSCi39lkCKD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the two datasets\n",
        "augmented_val_dataset = ConcatDataset([val_dataset, transformed_val])"
      ],
      "metadata": {
        "id": "lF01rpcRkYVM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloader \n",
        "train_loader = DataLoader(augmented_train_dataset, batch_size=128,\n",
        "                          shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(augmented_val_dataset, batch_size=128, \n",
        "                        shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, \n",
        "                         shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "Y1bfl7YakYhD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train, label in train_loader:\n",
        "    print(train.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVE82Tp6zIXz",
        "outputId": "df2448a9-4c4c-4040-820a-dfc71aad55c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 200, 200, 3])\n"
          ]
        }
      ]
    }
  ]
}