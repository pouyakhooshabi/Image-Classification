{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLrn6NbZQ-zD",
        "outputId": "b2f83079-4a6d-495c-b3a1-8d09a6c81b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import os\n",
        "import cv2 \n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "from skimage.transform import rotate, AffineTransform\n",
        "from skimage.util import random_noise\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import TensorDataset, ConcatDataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8nCBRoTYdle1",
        "outputId": "dca818b3-19fd-4422-913f-69b888c4fcb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "rQfhsRgWSGTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This class reads the data for each dataset\n",
        "class readDataset:\n",
        "    def __init__(self, num_classes, path):\n",
        "        self.height = 200\n",
        "        self.width = 200\n",
        "        self.channels = 3\n",
        "        self.path = path\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def DogsAndCats(self):\n",
        "        class_names = ['dogs', 'cats']\n",
        "        # two lists containing data train\n",
        "        train_data_=[]\n",
        "        train_labels_=[]\n",
        "        # two lists containing data test\n",
        "        test_data_=[]\n",
        "        test_labels_=[]\n",
        "        # read path names of dataset\n",
        "        for i in range(self.num_classes) :\n",
        "            path_train = self.path + \"training_set/\" + class_names[i] \n",
        "            path_test = self.path + \"test_set/\" + class_names[i]\n",
        "            data_names_train = os.listdir(path_train)\n",
        "            data_names_test = os.listdir(path_test)\n",
        "            print(\"\\n class: \",i)\n",
        "            print(\"number of train data: \",len(data_names_train))\n",
        "            print('--------------------')\n",
        "            print(\"number of test data: \",len(data_names_test))\n",
        "            # read train data\n",
        "            for j in range(len(data_names_train)):\n",
        "                name = data_names_train[j]\n",
        "                path_to_image = path_train + \"/\" + name\n",
        "                image = cv2.imread(path_to_image)\n",
        "                image = cv2.resize(image, (self.height, self.width))\n",
        "                train_data_.append(np.array(image))\n",
        "                train_labels_.append(i)\n",
        "            # read test set data        \n",
        "            for k in range(len(data_names_test)):\n",
        "                name = data_names_test[k]\n",
        "                path_to_image = path_test + \"/\" + name\n",
        "                image = cv2.imread(path_to_image)\n",
        "                image = cv2.resize(image, (self.height, self.width))\n",
        "                test_data_.append(np.array(image))\n",
        "                test_labels_.append(i)\n",
        "        # return two lists\n",
        "        train_data_ = np.array(train_data_)\n",
        "        test_data_ = np.array(test_data_)\n",
        "\n",
        "        train_labels_ = to_categorical(np.array(train_labels_), self.num_classes)\n",
        "        test_labels_ = to_categorical(np.array(test_labels_), self.num_classes)\n",
        "        return train_data_, train_labels_, test_data_, test_labels_\n",
        "\n"
      ],
      "metadata": {
        "id": "bScbFrSETP7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/projectDataset/dogsAndCats/dataset/\"\n",
        "data_dogs_cats = readDataset(2, dataset_path)\n",
        "train_data, train_labels, test_data, test_labels = data_dogs_cats.DogsAndCats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bC52dilV_Oq",
        "outputId": "a3eac4b5-42ba-469b-964a-fc66b4d36508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " class:  0\n",
            "number of train data:  4033\n",
            "--------------------\n",
            "number of test data:  1000\n",
            "\n",
            " class:  1\n",
            "number of train data:  4009\n",
            "--------------------\n",
            "number of test data:  1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is saving the train and test data in to the drive\n",
        "path_save = '/content/drive/MyDrive/projectDataset/dogsAndCats/dataset/numpyFiles/'\n",
        "np.save(path_save+'catDogs_train_data.npy',train_data)\n",
        "np.save(path_save+'catDogs_train_labels.npy',train_labels)\n",
        "np.save(path_save+'catDogs_test_data.npy',test_data)\n",
        "np.save(path_save+ 'catDogs_test_labels.npy',test_labels)"
      ],
      "metadata": {
        "id": "ElTEDH_3cHQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is loading the numpies from the drive\n",
        "# uncomment in case\n",
        "\n",
        "# path_save = '/content/drive/MyDrive/projectDataset/dogsAndCats/dataset/numpyFiles/'\n",
        "# train_data = np.load(path_save+'catDogs_train_data.npy')\n",
        "# train_labels = np.load(path_save+'catDogs_train_labels.npy')\n",
        "# test_data = np.load(path_save+'catDogs_test_data.npy')\n",
        "# test_labels = np.load(path_save+'catDogs_test_labels.npy')"
      ],
      "metadata": {
        "id": "SIVWRooPmWjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the data\n",
        "# train_data = train_data/255.0\n",
        "# test_data = test_data/255.0"
      ],
      "metadata": {
        "id": "pFYtE5XTUp1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split train to train and val\n",
        "train_data, val_data, train_labels, val_label = train_test_split(train_data, \n",
        "                                                                 train_labels, \n",
        "                                                                 test_size=0.2)"
      ],
      "metadata": {
        "id": "H0YzKplej9q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Augmentation in Pytorch"
      ],
      "metadata": {
        "id": "bKfdMkodjucn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloader pytorch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "# convert numpy to tensors\n",
        "train_data = torch.from_numpy(train_data.astype(np.float32))\n",
        "test_data = torch.from_numpy(test_data.astype(np.float32))\n",
        "val_data = torch.from_numpy(val_data.astype(np.float32))\n",
        "train_labels = torch.from_numpy(train_labels.astype(np.float32))\n",
        "test_labels = torch.from_numpy(test_labels.astype(np.float32))\n",
        "val_labels = torch.from_numpy(val_label.astype(np.float32))\n",
        "# create tensor dataset\n",
        "train_dataset = TensorDataset(train_data, train_labels)\n",
        "val_dataset = TensorDataset(val_data, val_labels)\n",
        "test_dataset = TensorDataset(test_data, test_labels)"
      ],
      "metadata": {
        "id": "NLqVbcTvjznx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomRotation(degrees=45),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15)),\n",
        "    # transforms.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "papEtZjvjzsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this list includes k random number within the range of train length\n",
        "train_random_list = random.sample(range(0, len(train_data)), round(0.5 * len(train_data)))\n",
        "val_random_list = random.sample(range(0, len(val_data)), round(0.5 * len(val_data)))"
      ],
      "metadata": {
        "id": "L501_SbKjzul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply transformations to each sample in the dataset\n",
        "transformed_train = []\n",
        "cnt = 0\n",
        "for sample, label in train_dataset:\n",
        "    if cnt in train_random_list:\n",
        "      transformed_sample = transform(sample.permute(2, 0, 1))\n",
        "      # print(transformed_sample.shape)\n",
        "      transformed_train.append((transformed_sample.permute(1, 2, 0), label))\n",
        "    cnt += 1"
      ],
      "metadata": {
        "id": "DlW8G7hUjzw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the two datasets\n",
        "augmented_train_dataset = ConcatDataset([train_dataset, transformed_train])"
      ],
      "metadata": {
        "id": "_i4-5GxWkCI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply transformations to each sample in the val dataset\n",
        "transformed_val = []\n",
        "cnt = 0\n",
        "for sample, label in val_dataset:\n",
        "    if cnt in val_random_list:\n",
        "      transformed_sample = transform(sample.permute(2, 0, 1))\n",
        "      # print(transformed_sample.shape)\n",
        "      transformed_val.append((transformed_sample.permute(1, 2, 0), label))\n",
        "    cnt += 1"
      ],
      "metadata": {
        "id": "pzSCi39lkCKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the two datasets\n",
        "augmented_val_dataset = ConcatDataset([val_dataset, transformed_val])"
      ],
      "metadata": {
        "id": "lF01rpcRkYVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloader \n",
        "train_loader = DataLoader(augmented_train_dataset, batch_size=128,\n",
        "                          shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(augmented_val_dataset, batch_size=128, \n",
        "                        shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, \n",
        "                         shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "Y1bfl7YakYhD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}