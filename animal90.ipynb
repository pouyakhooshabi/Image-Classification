{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBIZlHNr7mMc"
      },
      "source": [
        "#Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLrn6NbZQ-zD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import os\n",
        "import cv2 \n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "from skimage.transform import rotate, AffineTransform\n",
        "from skimage.util import random_noise\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.utils.data import TensorDataset, ConcatDataset\n",
        "import random\n",
        "from torchvision import datasets\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nCBRoTYdle1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38085aa8-1e81-4540-e20b-63eb11ae7674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqmGyu9bEt6j"
      },
      "source": [
        "#Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuoWJNwbErsu"
      },
      "outputs": [],
      "source": [
        "# this cell is regarding the aumentation and normalization with the stats of data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(degrees=45),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2)),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15)),\n",
        "    # transforms.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2cA7x-tEyVg"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder(\"/content/drive/MyDrive/projectDataset/animal90/animals\", transform= transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcWUHw5fE0I3"
      },
      "outputs": [],
      "source": [
        "# this cell is spliting dataset to train and test\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "test_split = 0.2\n",
        "dataset_size = len(dataset)\n",
        "test_size = int(test_split * dataset_size)\n",
        "train_size = dataset_size - test_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFAC9xndE1-0"
      },
      "outputs": [],
      "source": [
        "# this cell is spliting train to train and val\n",
        "val_split = 0.2\n",
        "train_size = len(train_dataset)\n",
        "val_size = int(val_split * train_size)\n",
        "train_size = train_size - val_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VDElqbrE3kX"
      },
      "outputs": [],
      "source": [
        "# Define the data loaders for the train and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle= True,\n",
        "                          drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle= True,\n",
        "                        drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle= True,\n",
        "                         drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac8h6RKsjmK0"
      },
      "outputs": [],
      "source": [
        "num_class = len(dataset.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8-66uRij2O8",
        "outputId": "d5c26933-8f29-4be2-90bf-10afa0745a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85\n"
          ]
        }
      ],
      "source": [
        "print(num_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjzId0-j6Lax"
      },
      "source": [
        "#Finetune resnet 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oft7MQN86JTx",
        "outputId": "bc4178ee-3b9a-4631-98a4-7a22cfb788f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Define the ResNet18 model\n",
        "model = torchvision.models.resnet18(pretrained=False)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_class)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp0qXygn6N64"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJInl4m46QqH",
        "outputId": "814e6a8a-ea9a-4561-8826-e423af380ab4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the train images after 1 epochs is: 1.375\n",
            "Accuracy of the network on the val images after 1 epochs is: 1.171875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 1/50 [21:03<17:11:30, 1263.08s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images after 1 epochs is: 1.171875\n",
            "------------------------\n",
            "Accuracy of the network on the train images after 2 epochs is: 2.5625\n",
            "Accuracy of the network on the val images after 2 epochs is: 2.9947916666666665\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 2/50 [42:16<16:55:31, 1269.41s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images after 2 epochs is: 3.22265625\n",
            "------------------------\n",
            "Accuracy of the network on the train images after 3 epochs is: 3.6875\n",
            "Accuracy of the network on the val images after 3 epochs is: 3.90625\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 3/50 [1:02:00<16:03:43, 1230.28s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images after 3 epochs is: 4.4921875\n",
            "------------------------\n"
          ]
        }
      ],
      "source": [
        "# Train phase\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 50\n",
        "loss_train = []\n",
        "loss_val = []\n",
        "loss_test = []\n",
        "\n",
        "accuracy_train = []\n",
        "accuracy_val = []\n",
        "accuracy_test = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    running_loss_train = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    running_loss_train = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # loss\n",
        "        running_loss_train += loss.item()\n",
        "        # accuracy\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "    accuracy = 100 * correct_train / total_train\n",
        "    loss_train.append(running_loss_train/len(train_loader))\n",
        "    accuracy_train.append(accuracy)\n",
        "    print(f\"Accuracy of the network on the train images after {epoch + 1} epochs is: {accuracy}\")\n",
        "\n",
        "    # Evaluate the model on validation\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    running_loss_val = 0\n",
        "    with torch.no_grad():\n",
        "        for val_data in val_loader:\n",
        "            val_inputs, val_labels = val_data\n",
        "            val_inputs = val_inputs.to(DEVICE)\n",
        "            val_labels = val_labels.to(DEVICE)\n",
        "            # loss\n",
        "            val_outputs = model(val_inputs)\n",
        "            loss = criterion(val_outputs, val_labels)\n",
        "            running_loss_val += loss.item()\n",
        "            # accuracy\n",
        "            _, predicted_val = torch.max(val_outputs.data, 1)\n",
        "            total_val += val_labels.size(0)\n",
        "            correct_val += (predicted_val == val_labels).sum().item()\n",
        "    accuracy = 100 * correct_val / total_val\n",
        "    loss_val.append(running_loss_val/len(val_loader))        \n",
        "    accuracy_val.append(accuracy)\n",
        "    print(f\"Accuracy of the network on the val images after {epoch + 1} epochs is: {accuracy}\")\n",
        "\n",
        "    # Evaluate the model on test\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    running_loss_test = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(val_outputs, val_labels)\n",
        "            running_loss_test += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct_test / total_test\n",
        "    loss_test.append(running_loss_test/len(test_loader)) \n",
        "    accuracy_test.append(accuracy)\n",
        "    print(f\"Accuracy of the network on the test images after {epoch + 1} epochs is: {accuracy}\")\n",
        "    print('------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw9HwBbdXH0c"
      },
      "outputs": [],
      "source": [
        "#save the path\n",
        "model_save_name = 'resnet18.pt'\n",
        "path = f\"/content/drive/MyDrive/projectDataset/animal90/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-G4Z-eJ6S86"
      },
      "outputs": [],
      "source": [
        "# plot train and val and test accuracy \n",
        "plt.figure(figsize=(12,10), dpi=80)\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(loss_train, label=\"Train\")\n",
        "plt.plot(loss_val, color='red', label=\"Validation\")\n",
        "plt.title('Loss function for the training data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(loss_test)\n",
        "plt.title('Loss function for the test data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(accuracy_train, label=\"Train\")\n",
        "plt.plot(accuracy_val, color='red', label=\"Validation\")\n",
        "plt.title('Accuracy for the training data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(accuracy_test)\n",
        "plt.title('Accuracy for the test data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwjEO_rLe9bg"
      },
      "outputs": [],
      "source": [
        "# save accuracy and loss\n",
        "import pickle\n",
        "with open(\"/content/drive/MyDrive/projectDataset/animal90/accuracy_resnet18.txt\", \"wb\") as fp:\n",
        "    pickle.dump(accuracy_train, fp)\n",
        "    pickle.dump(accuracy_val, fp)\n",
        "    pickle.dump(accuracy_test, fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/projectDataset/animal90/loss_resnet18.txt\", \"wb\") as fp:\n",
        "    pickle.dump(loss_train, fp)\n",
        "    pickle.dump(loss_val, fp)\n",
        "    pickle.dump(loss_test, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyfrGoUl-Ncr"
      },
      "source": [
        "#ShuffleNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-GV5Pqn-N3i"
      },
      "outputs": [],
      "source": [
        "# Define the ShuflleNetV2 model\n",
        "model = torchvision.models.shufflenet_v2_x1_0(pretrained=False)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 90)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw0CEl6k-RE4"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM_QGnWe-Wo4"
      },
      "outputs": [],
      "source": [
        "# Train phase\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 50\n",
        "loss_train = []\n",
        "loss_val = []\n",
        "loss_test = []\n",
        "\n",
        "accuracy_train = []\n",
        "accuracy_val = []\n",
        "accuracy_test = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    running_loss_train = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    running_loss_train = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # loss\n",
        "        running_loss_train += loss.item()\n",
        "        # accuracy\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "    accuracy = 100 * correct_train / total_train\n",
        "    loss_train.append(running_loss_train/len(train_loader))\n",
        "    accuracy_train.append(accuracy)\n",
        "    print(f\"Accuracy of the network on the train images after {epoch + 1} epochs is: {accuracy}\")\n",
        "\n",
        "    # Evaluate the model on validation\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    running_loss_val = 0\n",
        "    with torch.no_grad():\n",
        "        for val_data in val_loader:\n",
        "            val_inputs, val_labels = val_data\n",
        "            val_inputs = val_inputs.to(DEVICE)\n",
        "            val_labels = val_labels.to(DEVICE)\n",
        "            # loss\n",
        "            val_outputs = model(val_inputs)\n",
        "            loss = criterion(val_outputs, val_labels)\n",
        "            running_loss_val += loss.item()\n",
        "            # accuracy\n",
        "            _, predicted_val = torch.max(val_outputs.data, 1)\n",
        "            total_val += val_labels.size(0)\n",
        "            correct_val += (predicted_val == val_labels).sum().item()\n",
        "    accuracy = 100 * correct_val / total_val\n",
        "    loss_val.append(running_loss_val/len(val_loader))        \n",
        "    accuracy_val.append(accuracy)\n",
        "    print(f\"Accuracy of the network on the val images after {epoch + 1} epochs is: {accuracy}\")\n",
        "\n",
        "    # Evaluate the model on test\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    running_loss_test = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(val_outputs, val_labels)\n",
        "            running_loss_test += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct_test / total_test\n",
        "    loss_test.append(running_loss_test/len(test_loader)) \n",
        "    accuracy_test.append(accuracy)\n",
        "    print(f\"Accuracy of the network on the test images after {epoch + 1} epochs is: {accuracy}\")\n",
        "    print('------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSsDDxWcYAlk"
      },
      "outputs": [],
      "source": [
        "#save the path\n",
        "model_save_name = 'shufflenet_v2_x1_0.pt'\n",
        "path = f\"/content/drive/MyDrive/projectDataset/animal90/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI1kQK08-alX"
      },
      "outputs": [],
      "source": [
        "# plot train and val and test accuracy \n",
        "plt.figure(figsize=(12,10), dpi=80)\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(loss_train, label=\"Train\")\n",
        "plt.plot(loss_val, color='red', label=\"Validation\")\n",
        "plt.title('Loss function for the training data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(loss_test)\n",
        "plt.title('Loss function for the test data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(accuracy_train, label=\"Train\")\n",
        "plt.plot(accuracy_val, color='red', label=\"Validation\")\n",
        "plt.title('Accuracy for the training data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(accuracy_test)\n",
        "plt.title('Accuracy for the test data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwxcAXwce2UV"
      },
      "outputs": [],
      "source": [
        "# save accuracy and loss\n",
        "import pickle\n",
        "with open(\"/content/drive/MyDrive/projectDataset/animal90/accuracy_shufflenet_v2_x1_0.txt\", \"wb\") as fp:\n",
        "    pickle.dump(accuracy_train, fp)\n",
        "    pickle.dump(accuracy_val, fp)\n",
        "    pickle.dump(accuracy_test, fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/projectDataset/animal90/loss_shufflenet_v2_x1_0.txt\", \"wb\") as fp:\n",
        "    pickle.dump(loss_train, fp)\n",
        "    pickle.dump(loss_val, fp)\n",
        "    pickle.dump(loss_test, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTEi-DLhFP4I"
      },
      "source": [
        "#MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7-VAmPtFR3A"
      },
      "outputs": [],
      "source": [
        "# Define the ShuflleNetV2 model\n",
        "model = torchvision.models.mobilenet_v2(pretrained=False)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 90)\n",
        "# num_ftrs = model.fc.in_features\n",
        "# model.fc = nn.Linear(num_ftrs, 2)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbKJdKVlFg4b"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O3xyMdBFiqm"
      },
      "outputs": [],
      "source": [
        "# Train phase\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 50\n",
        "loss_train = []\n",
        "loss_val = []\n",
        "loss_test = []\n",
        "\n",
        "accuracy_train = []\n",
        "accuracy_val = []\n",
        "accuracy_test = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    running_loss_train = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    running_loss_train = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # loss\n",
        "        running_loss_train += loss.item()\n",
        "        # accuracy\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "    accuracy = 100 * correct_train / total_train\n",
        "    loss_train.append(running_loss_train/len(train_loader))\n",
        "    accuracy_train.append(accuracy)\n",
        "    print(f\"Accuracy of the network on the train images after {epoch + 1} epochs is: {accuracy}\")\n",
        "\n",
        "    # Evaluate the model on validation\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    running_loss_val = 0\n",
        "    with torch.no_grad():\n",
        "        for val_data in val_loader:\n",
        "            val_inputs, val_labels = val_data\n",
        "            val_inputs = val_inputs.to(DEVICE)\n",
        "            val_labels = val_labels.to(DEVICE)\n",
        "            # loss\n",
        "            val_outputs = model(val_inputs)\n",
        "            loss = criterion(val_outputs, val_labels)\n",
        "            running_loss_val += loss.item()\n",
        "            # accuracy\n",
        "            _, predicted_val = torch.max(val_outputs.data, 1)\n",
        "            total_val += val_labels.size(0)\n",
        "            correct_val += (predicted_val == val_labels).sum().item()\n",
        "    accuracy = 100 * correct_val / total_val\n",
        "    loss_val.append(running_loss_val/len(val_loader))        \n",
        "    accuracy_val.append(accuracy)\n",
        "    print(f\"Accuracy of the network on the val images after {epoch + 1} epochs is: {accuracy}\")\n",
        "\n",
        "    # Evaluate the model on test\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    running_loss_test = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(val_outputs, val_labels)\n",
        "            running_loss_test += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct_test / total_test\n",
        "    loss_test.append(running_loss_test/len(test_loader)) \n",
        "    accuracy_test.append(accuracy)\n",
        "    print(f\"Accuracy of the network on the test images after {epoch + 1} epochs is: {accuracy}\")\n",
        "    print('------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOXXVd31YIVy"
      },
      "outputs": [],
      "source": [
        "#save the path\n",
        "model_save_name = 'mobilenet_v2.pt'\n",
        "path = f\"/content/drive/MyDrive/projectDataset/animal90/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn6_C-7LFkep"
      },
      "outputs": [],
      "source": [
        "# plot train and val and test accuracy \n",
        "plt.figure(figsize=(12,10), dpi=80)\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(loss_train, label=\"Train\")\n",
        "plt.plot(loss_val, color='red', label=\"Validation\")\n",
        "plt.title('Loss function for the training data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(loss_test)\n",
        "plt.title('Loss function for the test data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(accuracy_train, label=\"Train\")\n",
        "plt.plot(accuracy_val, color='red', label=\"Validation\")\n",
        "plt.title('Accuracy for the training data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(accuracy_test)\n",
        "plt.title('Accuracy for the test data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCrt1xAremPa"
      },
      "outputs": [],
      "source": [
        "# save accuracy and loss\n",
        "import pickle\n",
        "with open(\"/content/drive/MyDrive/projectDataset/animal90/accuracy_mobilenet_v2.txt\", \"wb\") as fp:\n",
        "    pickle.dump(accuracy_train, fp)\n",
        "    pickle.dump(accuracy_val, fp)\n",
        "    pickle.dump(accuracy_test, fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/projectDataset/animal90/loss_mobilenet_v2.txt\", \"wb\") as fp:\n",
        "    pickle.dump(loss_train, fp)\n",
        "    pickle.dump(loss_val, fp)\n",
        "    pickle.dump(loss_test, fp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qqmGyu9bEt6j",
        "sjzId0-j6Lax",
        "JyfrGoUl-Ncr",
        "kTEi-DLhFP4I"
      ],
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}